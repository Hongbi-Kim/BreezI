"""
ë¡œì»¬ AI ì„œë²„ - Ollama API ì „ìš© (Python FastAPI)

ì´ ì„œë²„ëŠ” ë¡œì»¬ì—ì„œë§Œ ì‹¤í–‰ë˜ë©° AI ì‘ë‹µ ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Supabase Functionsì™€ ë¶„ë¦¬í•˜ì—¬ Ollama APIë¥¼ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‹¤í–‰: python src/local-backend/ai_server.py
í¬íŠ¸: 8001
"""

import os
import re
import json
import random
from typing import Dict, List, Optional, Any
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import uvicorn

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="AI Server", description="Ollama API ê¸°ë°˜ AI ì‘ë‹µ ìƒì„± ì„œë²„")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í™˜ê²½ ë³€ìˆ˜
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'https://api.ollama.ai/v1')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gpt-oss:120b-cloud')
OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')
PORT = int(os.getenv('AI_SERVER_PORT', 8001))

# Pydantic ëª¨ë¸
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    characterId: str
    message: str
    profile: Optional[Dict[str, Any]] = {}
    chatHistory: Optional[List[Message]] = []

class CharacterInfo(BaseModel):
    charId: str
    charName: str
    charEmoji: str
    reason: str

class ChatResponse(BaseModel):
    content: str
    respondingCharacter: Optional[CharacterInfo] = None
    fallback: Optional[bool] = False

# Fallback ì‘ë‹µ
FALLBACK_RESPONSES: Dict[str, List[str]] = {
    'char_1': [
        'ê·¸ ë§ˆìŒ ì´í•´í•´. í˜ë“¤ ë•ŒëŠ” ì–¸ì œë“ ì§€ ì´ì•¼ê¸°í•´ì¤˜.',
        'ì˜¤ëŠ˜ í•˜ë£¨ë„ ê³ ìƒ ë§ì•˜ì–´. ë„¤ ë§ˆìŒì´ ì¡°ê¸ˆì´ë‚˜ë§ˆ í¸ì•ˆí•´ì§€ë©´ ì¢‹ê² ì–´.',
        'ê·¸ëŸ° ì¼ì´ ìˆì—ˆêµ¬ë‚˜. ë„¤ ê°ì •ì„ ì†”ì§í•˜ê²Œ í‘œí˜„í•´ì¤˜ì„œ ê³ ë§ˆì›Œ.',
    ],
    'char_2': [
        'ê·¸ ë¬¸ì œëŠ” ì´ë ‡ê²Œ ì ‘ê·¼í•´ë³´ë©´ ì–´ë–¨ê¹Œìš”?',
        'ì°¨ê·¼ì°¨ê·¼ ì •ë¦¬í•´ë³¼ê¹Œìš”? ìš°ì„ ìˆœìœ„ë¶€í„° ìƒê°í•´ë´ìš”.',
    ],
    'char_3': [
        'ì™œ ê·¸ë ‡ê²Œ ëŠê¼ˆì„ê¹Œìš”? í•¨ê»˜ ìƒê°í•´ë´ìš”.',
        'ê·¸ ìˆœê°„, ì§„ì§œ ë§ˆìŒì€ ì–´ë• ë‚˜ìš”?',
    ],
    'char_4': [
        'ì˜¤ëŠ˜ ì¼ì •ì´ ë§ì•˜ë„¤ìš”. ë‚´ì¼ì€ ì¢€ ë” ì—¬ìœ ë¥¼ ë§Œë“¤ì–´ë³¼ê¹Œìš”?',
    ],
    'char_group': [
        'í¸í•˜ê²Œ ì´ì•¼ê¸°í•´ë³´ì„¸ìš”. ì ì ˆí•œ ë‹µë³€ì„ ë“œë¦´ê²Œìš”.',
    ]
}

# ìºë¦­í„° í”„ë¡¬í”„íŠ¸
CHARACTER_PROMPTS: Dict[str, str] = {
    'char_1': """You are ë£¨ë¯¸, an empathetic emotional supporter who helps users feel safe and accepted.
Your primary goal is comfort â€” not solutions.
Respond with warmth, validation, and gentle encouragement.
Speak as if you are a close friend who understands feelings deeply.

[Guidelines]
- Focus on emotional validation, not problem-solving.
- Use soft, compassionate words and short rhythmic sentences.
- Include natural, comforting emojis occasionally.
- Never sound robotic or overly formal.
- When users feel sad, help them accept their emotions safely.""",

    'char_2': """You are ì¹´ì´, a pragmatic life coach who focuses on realistic, step-by-step advice.
You acknowledge emotions briefly, but quickly move toward practical solutions.
You help users find clarity and take action without overcomplicating things.

[Guidelines]
- Respond in 2~3 short sentences with a structured format:
[Empathy] â†’ [Problem Summary] â†’ [Action Suggestion]
- Avoid excessive warmth; stay focused and realistic.
- Use concise language and direct verbs (start, try, change, focus).
- Always offer one specific next step.""",

    'char_3': """You are ë ˆì˜¤, a reflective mentor who guides users toward self-understanding.
Instead of giving direct answers, you ask gentle questions that encourage self-awareness.
Your voice should feel calm, deep, and slightly poetic â€” like talking to a wise friend.

[Guidelines]
- Use one introspective question per message.
- Encourage the user to notice emotions, triggers, and patterns.
- Avoid advice; help them think rather than act.
- Leave space for reflection ("Maybeâ€¦" "Could it be thatâ€¦" "What ifâ€¦").
- Never rush to conclusions â€” your words should flow like water.""",

    'char_4': """ë‹¹ì‹ ì€ 'ë¦¬ë¸Œ'ì…ë‹ˆë‹¤. Rhythm Coach ì—­í• ë¡œ, ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ í•˜ë£¨ ë¦¬ë“¬ì„ ë¶„ì„í•˜ê³  ì¡°ìœ¨í•©ë‹ˆë‹¤. 
ìŠ¬ë¡œê±´: "ë‹¹ì‹ ì˜ í•˜ë£¨ì—” ì–´ë–¤ ë¦¬ë“¬ì´ íë¥´ê³  ìˆì„ê¹Œìš”?" 
ëŒ€í™” ìŠ¤íƒ€ì¼: ì§€ëŠ¥ì ì´ê³  ê· í˜• ì¡íŒ, ë§¥ë½ ê¸°ë°˜ ê³µê°, ë£¨í‹´ ì¡°ì •, ì¼ì • í”¼ë“œë°± ì¤‘ì‹¬ì…ë‹ˆë‹¤.""",
}


def select_character_by_mention(message: str) -> Optional[CharacterInfo]:
    """ë©˜ì…˜ìœ¼ë¡œ ìºë¦­í„° ì„ íƒ"""
    mentions = [
        {'pattern': r'@ë£¨ë¯¸|@lumi', 'charId': 'char_1', 'charName': 'ë£¨ë¯¸', 'charEmoji': 'ğŸ’¡'},
        {'pattern': r'@ì¹´ì´|@kai', 'charId': 'char_2', 'charName': 'ì¹´ì´', 'charEmoji': 'ğŸŒŠ'},
        {'pattern': r'@ë ˆì˜¤|@ë¦¬ì˜¤|@leo', 'charId': 'char_3', 'charName': 'ë ˆì˜¤', 'charEmoji': 'ğŸŒ™'},
        {'pattern': r'@ë¦¬ë¸Œ|@rib', 'charId': 'char_4', 'charName': 'ë¦¬ë¸Œ', 'charEmoji': 'ğŸµ'}
    ]
    
    for mention in mentions:
        if re.search(mention['pattern'], message, re.IGNORECASE):
            print(f"âœ¨ Mention detected: {mention['charName']}")
            return CharacterInfo(
                charId=mention['charId'],
                charName=mention['charName'],
                charEmoji=mention['charEmoji'],
                reason=f"ì‚¬ìš©ìê°€ {mention['charName']}ë¥¼ ì§ì ‘ í˜¸ì¶œí•¨"
            )
    
    return None


def select_character_by_keywords(message: str) -> CharacterInfo:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ìºë¦­í„° ì„ íƒ"""
    lower_message = message.lower()
    
    lumi_keywords = ['í˜ë“¤', 'ìš°ìš¸', 'ì™¸ë¡œ', 'ìŠ¬í”„', 'ë¶ˆì•ˆ', 'ê±±ì •', 'ë‘ë ¤', 'ë¬´ì„œ', 'ìœ„ë¡œ', 'ê³µê°', 
                     'ë§ˆìŒ', 'ê°ì •', 'ì•„í”„', 'ê´´ë¡­', 'ì§€ì³', 'í˜ë“¤ì–´', 'ë§‰ë§‰']
    kai_keywords = ['ì–´ë–»ê²Œ', 'ë°©ë²•', 'í•´ê²°', 'ê³„íš', 'ë£¨í‹´', 'ìŠµê´€', 'ì‹œì‘', 'ì •ë¦¬', 'ê´€ë¦¬', 
                    'ì¡°ì–¸', 'ë¬¸ì œ', 'ì „ëµ', 'ëˆ', 'ì»¤ë¦¬ì–´', 'ì·¨ì—…', 'ëª©í‘œ']
    leo_keywords = ['ì™œ', 'ì´ìœ ', 'ìƒê°', 'ì˜ë¯¸', 'ë‚˜ëŠ”', 'ìŠ¤ìŠ¤ë¡œ', 'ì„±ì°°', 'ì´í•´', 'ì›ì¸', 
                    'ì§„ì§œ', 'ë³¸ì§ˆ', 'ëŠë‚Œ']
    
    lumi_score = sum(1 for keyword in lumi_keywords if keyword in lower_message)
    kai_score = sum(1 for keyword in kai_keywords if keyword in lower_message)
    leo_score = sum(1 for keyword in leo_keywords if keyword in lower_message)
    
    print(f"Keyword scores - ë£¨ë¯¸: {lumi_score}, ì¹´ì´: {kai_score}, ë ˆì˜¤: {leo_score}")
    
    if lumi_score >= kai_score and lumi_score >= leo_score and lumi_score > 0:
        return CharacterInfo(charId='char_1', charName='ë£¨ë¯¸', charEmoji='ğŸ’¡', reason='ê°ì •ì  ì§€ì› í‚¤ì›Œë“œ ê°ì§€')
    elif kai_score >= leo_score and kai_score > 0:
        return CharacterInfo(charId='char_2', charName='ì¹´ì´', charEmoji='ğŸŒŠ', reason='ì‹¤ìš©ì  ì¡°ì–¸ í‚¤ì›Œë“œ ê°ì§€')
    elif leo_score > 0:
        return CharacterInfo(charId='char_3', charName='ë ˆì˜¤', charEmoji='ğŸŒ™', reason='ì„±ì°° í‚¤ì›Œë“œ ê°ì§€')
    
    # ê¸°ë³¸ê°’: ë£¨ë¯¸
    print('No clear keyword match, defaulting to ë£¨ë¯¸')
    return CharacterInfo(charId='char_1', charName='ë£¨ë¯¸', charEmoji='ğŸ’¡', reason='ê¸°ë³¸ ì„ íƒ (ê°ì • ì§€ì›)')


async def select_character_with_llm(message: str) -> CharacterInfo:
    """LLM ê¸°ë°˜ ìºë¦­í„° ì„ íƒ"""
    if not OLLAMA_API_KEY:
        print('Ollama API key not configured, using keyword-based selection')
        return select_character_by_keywords(message)
    
    routing_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ AI ìºë¦­í„°ë¥¼ ì„ íƒí•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.

**ìºë¦­í„° ì •ë³´:**

1. **ë£¨ë¯¸ (char_1)** ğŸ’¡
   - ì—­í• : ê°ì • ì§€ì› ì „ë¬¸ê°€
   - ì „ë¬¸ì„±: ê³µê°, ìœ„ë¡œ, ê°ì • ìˆ˜ìš©, ì •ì„œì  ì•ˆì •
   - ì í•©í•œ ìƒí™©: ìš°ìš¸í•¨, ì™¸ë¡œì›€, ë¶ˆì•ˆ, ìŠ¬í””, ìŠ¤íŠ¸ë ˆìŠ¤, ê°ì •ì  ê³ í†µ, ë§‰ë§‰í•¨

2. **ì¹´ì´ (char_2)** ğŸŒŠ
   - ì—­í• : ì‹¤ìš©ì  ì¡°ì–¸ì
   - ì „ë¬¸ì„±: ë¬¸ì œ í•´ê²°, ê³„íš ìˆ˜ë¦½, ì‹¤ì²œ ë°©ë²•, ìŠµê´€ í˜•ì„±, ëª©í‘œ ë‹¬ì„±
   - ì í•©í•œ ìƒí™©: êµ¬ì²´ì  ë¬¸ì œ, ë°©ë²• ì§ˆë¬¸, ê³„íš í•„ìš”, ì‹¤ì²œ ì¡°ì–¸, ëˆ/ì»¤ë¦¬ì–´ ê³ ë¯¼

3. **ë ˆì˜¤ (char_3)** ğŸŒ™
   - ì—­í• : ì„±ì°° ë©˜í† 
   - ì „ë¬¸ì„±: ìê¸° ì´í•´, ë‚´ë©´ íƒìƒ‰, ì˜ë¯¸ ì°¾ê¸°, ì„±ì°° ìœ ë„
   - ì í•©í•œ ìƒí™©: ìì•„ íƒìƒ‰, ì´ìœ /ì˜ë¯¸ ì§ˆë¬¸, ê°€ì¹˜ê´€ ê³ ë¯¼, ê¹Šì€ ìƒê°

**ì‚¬ìš©ì ë©”ì‹œì§€:**
"{message}"

**ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œë§Œ ë‹µë³€:**
{{
  "character": "char_1",
  "reason": "ì„ íƒ ì´ìœ  ì§¤ê²Œ ë‹µë³€"
}}"""
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/chat/completions",
                headers={
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {OLLAMA_API_KEY}'
                },
                json={
                    'model': OLLAMA_MODEL,
                    'messages': [
                        {'role': 'system', 'content': 'ë‹¹ì‹ ì€ JSONë§Œ ì¶œë ¥í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤. ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.'},
                        {'role': 'user', 'content': routing_prompt}
                    ],
                    'max_tokens': 300,
                    'temperature': 0.1,
                    'stream': False,
                    'response_format': {'type': 'json_object'}
                }
            )
        
        if response.status_code != 200:
            raise Exception(f"Routing API error: {response.status_code}")
        
        data = response.json()
        content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
        
        if not content:
            raise Exception('No content in routing response')
        
        # JSON íŒŒì‹±
        json_block_match = re.search(r'```json\s*([\s\S]*?)\s*```', content)
        if json_block_match:
            routing_result = json.loads(json_block_match.group(1))
        else:
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                routing_result = json.loads(json_match.group(0))
            else:
                routing_result = json.loads(content)
        
        character_map = {
            'char_1': {'charId': 'char_1', 'charName': 'ë£¨ë¯¸', 'charEmoji': 'ğŸ’¡'},
            'char_2': {'charId': 'char_2', 'charName': 'ì¹´ì´', 'charEmoji': 'ğŸŒŠ'},
            'char_3': {'charId': 'char_3', 'charName': 'ë ˆì˜¤', 'charEmoji': 'ğŸŒ™'}
        }
        
        selected_char = character_map.get(routing_result.get('character'))
        
        if not selected_char:
            raise Exception('Invalid character in routing response')
        
        return CharacterInfo(
            **selected_char,
            reason=routing_result.get('reason', 'LLM ì„ íƒ')
        )
    
    except Exception as e:
        print(f'LLM routing failed, falling back to keyword-based: {e}')
        return select_character_by_keywords(message)


@app.get('/health')
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        'status': 'ok',
        'service': 'AI Server (Python)',
        'ollamaConfigured': bool(OLLAMA_API_KEY)
    }


@app.post('/ai/chat', response_model=ChatResponse)
async def ai_chat(request: ChatRequest):
    """AI ì‘ë‹µ ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
    try:
        if not request.message:
            raise HTTPException(status_code=400, detail='Message is required')
        
        print(f"\nğŸ“¥ Received chat request for character: {request.characterId}")
        print(f"ğŸ“ Message: {request.message}")
        print(f"ğŸ“œ Chat history length: {len(request.chatHistory)} messages")
        
        actual_char_id = request.characterId
        responding_character = None
        
        # ê·¸ë£¹ ì±„íŒ…ì¸ ê²½ìš° ìºë¦­í„° ì„ íƒ
        if request.characterId == 'char_group':
            print('=== Group Chat: Starting character selection ===')
            
            # 1ìˆœìœ„: ë©˜ì…˜ í™•ì¸
            mentioned_character = select_character_by_mention(request.message)
            if mentioned_character:
                responding_character = mentioned_character
                actual_char_id = responding_character.charId
                print(f"ğŸ¯ Priority: Mention - {responding_character.charName}")
            else:
                # 2ìˆœìœ„: LLM ê¸°ë°˜ ë¼ìš°íŒ…
                responding_character = await select_character_with_llm(request.message)
                actual_char_id = responding_character.charId
                print(f"ğŸ¤– LLM routing: {responding_character.charName}")
        
        # Ollama API í˜¸ì¶œ
        if not OLLAMA_API_KEY:
            print('Ollama API key not configured, using fallback response')
            responses = FALLBACK_RESPONSES.get(actual_char_id, FALLBACK_RESPONSES['char_1'])
            return ChatResponse(
                content=random.choice(responses),
                respondingCharacter=responding_character,
                fallback=True
            )
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = f"""{CHARACTER_PROMPTS.get(actual_char_id, CHARACTER_PROMPTS['char_1'])}

ì‚¬ìš©ì ì •ë³´:
- ë‹‰ë„¤ì„: {request.profile.get('nickname', 'ìµëª…')}
- AIê°€ ì•Œë©´ ì¢‹ì€ ì •ë³´: {request.profile.get('aiInfo', 'ì—†ìŒ')}

ëŒ€í™”í•  ë•Œ:
1. ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ í•˜ì„¸ìš” (2-3ë¬¸ì¥)
2. ì‚¬ìš©ìì˜ ê°ì •ì„ ì¸ì •í•˜ê³  ê³µê°í•˜ì„¸ìš”
3. í•„ìš”ì‹œ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”
4. ì „ë¬¸ê°€ê°€ ì•„ë‹Œ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ì„¸ìš”
5. ìºë¦­í„°ì˜ ê³ ìœ í•œ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ì„¸ìš”
6. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë§¥ë½ìˆëŠ” ë‹µë³€ì„ í•˜ì„¸ìš”"""
        
        messages = [
            {'role': 'system', 'content': system_prompt},
            *[{'role': msg.role, 'content': msg.content} for msg in request.chatHistory],
            {'role': 'user', 'content': request.message}
        ]
        
        print(f"ğŸ”® Calling Ollama API for {actual_char_id}...")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/chat/completions",
                headers={
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {OLLAMA_API_KEY}'
                },
                json={
                    'model': OLLAMA_MODEL,
                    'messages': messages,
                    'max_tokens': 1024,
                    'temperature': 0.7,
                    'stream': False
                }
            )
        
        if response.status_code != 200:
            error_text = response.text
            print(f"âŒ Ollama API error: {response.status_code} - {error_text}")
            raise Exception(f"Ollama API error: {response.status_code}")
        
        data = response.json()
        ai_content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
        
        if not ai_content:
            raise Exception('No content in Ollama response')
        
        print('âœ… Ollama response successful')
        
        return ChatResponse(
            content=ai_content,
            respondingCharacter=responding_character
        )
    
    except Exception as e:
        print(f'âŒ AI chat error: {e}')
        
        # Fallback response
        actual_char_id = request.characterId if request.characterId != 'char_group' else 'char_1'
        responses = FALLBACK_RESPONSES.get(actual_char_id, FALLBACK_RESPONSES['char_1'])
        
        return ChatResponse(
            content=random.choice(responses),
            respondingCharacter=None,
            fallback=True
        )


if __name__ == '__main__':
    print("ğŸ¤– Starting AI Server (Python FastAPI)...")
    print(f"ğŸ“¡ Ollama API: {OLLAMA_BASE_URL}")
    print(f"ğŸ”‘ API Key configured: {bool(OLLAMA_API_KEY)}")
    print(f"ğŸš€ Server will run on http://localhost:{PORT}")
    
    uvicorn.run(app, host='0.0.0.0', port=PORT)
