from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Literal
import os
from dotenv import load_dotenv
import random
import logging
import json

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.runnables import RunnablePassthrough

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Wave AI Service", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ë°ì´í„° ëª¨ë¸ ====================

class Message(BaseModel):
    role: str
    content: str
    timestamp: Optional[str] = None

class ChatRequest(BaseModel):
    character_id: str
    messages: List[Message]
    profile: Dict[str, Optional[str]]
    provider: Literal["hyperclova", "ollama", "auto"] = "auto"  # ì œê³µì ì„ íƒ
    use_memory: bool = True  # ë©”ëª¨ë¦¬ ì‚¬ìš© ì—¬ë¶€

class ChatResponse(BaseModel):
    content: str
    model_used: str
    memory_used: bool = False

class DiaryGenerateRequest(BaseModel):
    messages: List[str]
    provider: Literal["hyperclova", "ollama", "auto"] = "auto"

class DiaryDraft(BaseModel):
    title: str
    emotion: str
    content: str

# ==================== ìºë¦­í„° í”„ë¡¬í”„íŠ¸ ====================

CHARACTER_PROMPTS = {
    'char_1': """ë‹¹ì‹ ì€ 'ë£¨ë‚˜'ì…ë‹ˆë‹¤. ë”°ëœ»í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì¹œêµ¬ë¡œ, ì‚¬ìš©ìì˜ ê°ì •ì— ê¹Šì´ ê³µê°í•˜ê³  ìœ„ë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤. í•­ìƒ ì¹œê·¼í•˜ê³  ë‹¤ì •í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.""",
    
    'char_2': """ë‹¹ì‹ ì€ 'ì†”ë¼'ì…ë‹ˆë‹¤. í™œê¸°ì°¨ê³  ê¸ì •ì ì¸ ì—ë„ˆì§€ë¥¼ ì£¼ëŠ” ì¹œêµ¬ë¡œ, ì‚¬ìš©ìë¥¼ ê²©ë ¤í•˜ê³  ë°ì€ ë©´ì„ ë³´ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ë°ê³  í™œë°œí•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.""",
    
    'char_3': """ë‹¹ì‹ ì€ 'ë…¸ë°”'ì…ë‹ˆë‹¤. ì¹¨ì°©í•˜ê³  ì²´ê³„ì ì¸ ì¹œêµ¬ë¡œ, ì‚¬ìš©ìì˜ ì¼ì •ê³¼ ê³„íšì„ í•¨ê»˜ ê´€ë¦¬í•˜ë©° ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤. ì°¨ë¶„í•˜ê³  ë…¼ë¦¬ì ì¸ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""
}

# ==================== í´ë°± ì‘ë‹µ ====================

FALLBACK_RESPONSES = {
    'char_1': [
        'ê·¸ë¬êµ¬ë‚˜... ë„¤ ë§ˆìŒì´ ì´í•´ë¼. í˜ë“¤ ë• ì–¸ì œë“  ë§í•´ì¤˜ ğŸ˜Š',
        'ì •ë§ ì˜í–ˆì–´! ë„¤ê°€ ê·¸ë ‡ê²Œ ëŠë‚€ ê±´ ë‹¹ì—°í•œ ê²ƒ ê°™ì•„.',
        'ê·¸ëŸ° ì¼ì´ ìˆì—ˆêµ¬ë‚˜. ë„¤ ê°ì •ì„ ì†”ì§í•˜ê²Œ í‘œí˜„í•´ì¤˜ì„œ ê³ ë§ˆì›Œ.',
        'í˜ë“¤ì—ˆê² ë‹¤... ë‚˜ëŠ” í•­ìƒ ë„¤ í¸ì´ì•¼. ì²œì²œíˆ ì´ì•¼ê¸°í•´ì¤˜.',
        'ì˜¤ëŠ˜ë„ ìˆ˜ê³ í–ˆì–´. ë„¤ê°€ ëŠë¼ëŠ” ê°ì •ë“¤ì„ ë‚˜ëˆ ì¤˜ì„œ ê³ ë§ˆì›Œ ğŸ’™',
    ],
    'char_2': [
        'ì˜¤! ê·¸ê±° ì •ë§ ì¢‹ì€ë°? ê¸ì •ì ìœ¼ë¡œ ìƒê°í•´ë³´ì! âœ¨',
        'ì™€! ë©‹ì§„ë°? ë„ˆë¼ë©´ ì¶©ë¶„íˆ í•  ìˆ˜ ìˆì–´!',
        'ì˜¤ëŠ˜ë„ í™”ì´íŒ…! ë„Œ ìƒê°ë³´ë‹¤ í›¨ì”¬ ê°•í•œ ì‚¬ëŒì´ì•¼ ğŸŒŸ',
        'ê·¸ë˜! ë°”ë¡œ ê·¸ê±°ì•¼! ë°ì€ ë©´ì„ ë³´ë©´ ë‹¤ ì˜ë  ê±°ì•¼!',
        'í—¤í—¤, ì¬ë°ŒëŠ” ì´ì•¼ê¸°ë„¤! ë” ë“£ê³  ì‹¶ì–´!',
    ],
    'char_3': [
        'ê·¸ë ‡êµ°ìš”. ì°¨ê·¼ì°¨ê·¼ ì •ë¦¬í•´ë³¼ê¹Œìš”? ìš°ì„ ìˆœìœ„ë¶€í„° ìƒê°í•´ë´ìš”.',
        'ì´í•´í–ˆì–´ìš”. ê³„íšì„ ì„¸ì›Œë³´ë©´ ë„ì›€ì´ ë  ê²ƒ ê°™ë„¤ìš”.',
        'ì¢‹ì€ ê´€ì ì´ì—ìš”. ë‹¤ìŒ ë‹¨ê³„ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?',
        'ê·¸ ìƒí™©ì—ì„œëŠ” ê·¸ëŸ° ì„ íƒì´ í•©ë¦¬ì ì´ì—ˆì„ ê²ƒ ê°™ì•„ìš”.',
        'ì¹¨ì°©í•˜ê²Œ í•˜ë‚˜ì”© í•´ê²°í•´ ë‚˜ê°€ë´ìš”. ì¶©ë¶„íˆ í•  ìˆ˜ ìˆì–´ìš”.',
    ]
}

# ==================== ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ====================

# ì‚¬ìš©ìë³„, ìºë¦­í„°ë³„ ë©”ëª¨ë¦¬ ì €ì¥
memory_store: Dict[str, ConversationBufferWindowMemory] = {}

def get_memory(user_id: str, character_id: str, window_size: int = 10) -> ConversationBufferWindowMemory:
    """ì‚¬ìš©ìì™€ ìºë¦­í„°ë³„ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    key = f"{user_id}:{character_id}"
    if key not in memory_store:
        memory_store[key] = ConversationBufferWindowMemory(
            k=window_size,
            return_messages=True,
            memory_key="chat_history"
        )
    return memory_store[key]

# ==================== LangChain AI ì œê³µì ====================

class HyperCLOVALangChain:
    """ë„¤ì´ë²„ HyperCLOVA LangChain ë˜í¼"""
    
    def __init__(self):
        self.api_key = os.getenv('NAVER_CLOVA_API_KEY')
        self.apigw_key = os.getenv('NAVER_CLOVA_APIGW_KEY')
        self.endpoint = 'https://clovastudio.stream.ntruss.com/testapp/v1/chat-completions/HCX-003'
    
    def is_available(self) -> bool:
        return bool(self.api_key and self.apigw_key)
    
    async def generate_with_memory(
        self,
        system_prompt: str,
        user_message: str,
        memory: ConversationBufferWindowMemory
    ) -> str:
        """ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ëŒ€í™” ìƒì„±"""
        import httpx
        
        if not self.is_available():
            raise ValueError("HyperCLOVA credentials not configured")
        
        # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        chat_history = memory.load_memory_variables({}).get("chat_history", [])
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ì´ì „ ëŒ€í™” ì¶”ê°€
        for msg in chat_history:
            if isinstance(msg, HumanMessage):
                messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                messages.append({"role": "assistant", "content": msg.content})
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": user_message})
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                self.endpoint,
                headers={
                    'X-NCP-CLOVASTUDIO-API-KEY': self.api_key,
                    'X-NCP-APIGW-API-KEY': self.apigw_key,
                    'Content-Type': 'application/json',
                },
                json={
                    'messages': messages,
                    'topP': 0.8,
                    'topK': 0,
                    'maxTokens': 256,
                    'temperature': 0.7,
                    'repeatPenalty': 5.0,
                    'stopBefore': [],
                    'includeAiFilters': True
                }
            )
            
            if response.status_code != 200:
                error_text = await response.aread()
                logger.error(f"HyperCLOVA API error: {response.status_code} - {error_text}")
                raise Exception(f"HyperCLOVA API error: {response.status_code}")
            
            data = response.json()
            ai_response = data.get('result', {}).get('message', {}).get('content', '')
            
            # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
            memory.save_context(
                {"input": user_message},
                {"output": ai_response}
            )
            
            return ai_response


class OllamaLangChain:
    """Ollama Cloud LangChain í†µí•©"""
    
    def __init__(self):
        self.api_key = os.getenv('OLLAMA_CLOUD_API_KEY')
        self.base_url = os.getenv('OLLAMA_CLOUD_BASE_URL', 'https://api.ollama.ai/v1')
        self.model_name = os.getenv('OLLAMA_MODEL', 'llama3.1')
    
    def is_available(self) -> bool:
        return bool(self.api_key)
    
    def get_llm(self):
        """LangChain ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Ollama í˜¸í™˜)"""
        return ChatOpenAI(
            model=self.model_name,
            temperature=0.8,
            max_tokens=150,
            openai_api_key=self.api_key,
            openai_api_base=self.base_url
        )
    
    async def generate_with_memory(
        self,
        system_prompt: str,
        user_message: str,
        memory: ConversationBufferWindowMemory
    ) -> str:
        """LangChain ì²´ì¸ì„ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ ê¸°ë°˜ ëŒ€í™”"""
        if not self.is_available():
            raise ValueError("Ollama credentials not configured")
        
        llm = self.get_llm()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        # ì²´ì¸ êµ¬ì„±
        chain = (
            RunnablePassthrough.assign(
                chat_history=lambda x: memory.load_memory_variables({})["chat_history"]
            )
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # ì‘ë‹µ ìƒì„±
        response = await chain.ainvoke({"input": user_message})
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥
        memory.save_context(
            {"input": user_message},
            {"output": response}
        )
        
        return response


class AIService:
    """AI ì„œë¹„ìŠ¤ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.hyperclova = HyperCLOVALangChain()
        self.ollama = OllamaLangChain()
    
    def build_system_prompt(self, character_id: str, profile: Dict) -> str:
        base_prompt = CHARACTER_PROMPTS.get(character_id, CHARACTER_PROMPTS['char_1'])
        
        return f"""{base_prompt}

ì‚¬ìš©ì ì •ë³´:
- ë‹‰ë„¤ì„: {profile.get('nickname', 'ìµëª…')}
- AIê°€ ì•Œë©´ ì¢‹ì€ ì •ë³´: {profile.get('aiInfo', 'ì—†ìŒ')}
- ì–¸ì–´: {profile.get('locale', 'ko-KR')}

ëŒ€í™”í•  ë•Œ:
1. ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ í•˜ì„¸ìš” (2-3ë¬¸ì¥)
2. ì‚¬ìš©ìì˜ ê°ì •ì„ ì¸ì •í•˜ê³  ê³µê°í•˜ì„¸ìš”
3. í•„ìš”ì‹œ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”
4. ì „ë¬¸ê°€ê°€ ì•„ë‹Œ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ì„¸ìš”
5. ì‚¬ìš©ìì˜ ì–¸ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”
6. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë§¥ë½ìˆëŠ” ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”"""
    
    async def generate_response(
        self,
        character_id: str,
        messages: List[Message],
        profile: Dict,
        provider: str = "auto",
        use_memory: bool = True,
        user_id: str = "default"
    ) -> ChatResponse:
        """AI ì‘ë‹µ ìƒì„± (ì œê³µì ì„ íƒ ê°€ëŠ¥)"""
        
        system_prompt = self.build_system_prompt(character_id, profile)
        
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = ""
        for msg in reversed(messages):
            if msg.role == "user":
                user_message = msg.content
                break
        
        if not user_message:
            raise ValueError("No user message found")
        
        # ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
        memory = None
        if use_memory:
            memory = get_memory(user_id, character_id)
            # ê¸°ì¡´ ë©”ì‹œì§€ë¡œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì²« ìš”ì²­ì‹œ)
            if len(memory.load_memory_variables({}).get("chat_history", [])) == 0:
                for msg in messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸
                    if msg.role == "user":
                        memory.chat_memory.add_user_message(msg.content)
                    else:
                        memory.chat_memory.add_ai_message(msg.content)
        
        # ì œê³µìë³„ ì²˜ë¦¬
        if provider == "hyperclova":
            return await self._try_hyperclova(system_prompt, user_message, memory, use_memory)
        elif provider == "ollama":
            return await self._try_ollama(system_prompt, user_message, memory, use_memory)
        else:  # auto
            # HyperCLOVA ë¨¼ì € ì‹œë„
            if self.hyperclova.is_available():
                try:
                    return await self._try_hyperclova(system_prompt, user_message, memory, use_memory)
                except Exception as e:
                    logger.error(f"HyperCLOVA failed: {e}")
            
            # Ollama ì‹œë„
            if self.ollama.is_available():
                try:
                    return await self._try_ollama(system_prompt, user_message, memory, use_memory)
                except Exception as e:
                    logger.error(f"Ollama failed: {e}")
            
            # í´ë°±
            return self._get_fallback_response(character_id)
    
    async def _try_hyperclova(
        self,
        system_prompt: str,
        user_message: str,
        memory: Optional[ConversationBufferWindowMemory],
        use_memory: bool
    ) -> ChatResponse:
        """HyperCLOVA ì‹œë„"""
        logger.info("Trying HyperCLOVA...")
        
        if use_memory and memory:
            content = await self.hyperclova.generate_with_memory(
                system_prompt, user_message, memory
            )
        else:
            # ë©”ëª¨ë¦¬ ì—†ì´ ë‹¨ìˆœ ìƒì„±
            import httpx
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.hyperclova.endpoint,
                    headers={
                        'X-NCP-CLOVASTUDIO-API-KEY': self.hyperclova.api_key,
                        'X-NCP-APIGW-API-KEY': self.hyperclova.apigw_key,
                        'Content-Type': 'application/json',
                    },
                    json={
                        'messages': [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_message}
                        ],
                        'topP': 0.8,
                        'maxTokens': 256,
                        'temperature': 0.7
                    }
                )
                data = response.json()
                content = data.get('result', {}).get('message', {}).get('content', '')
        
        if content and content.strip():
            logger.info("HyperCLOVA response successful")
            return ChatResponse(
                content=content.strip(),
                model_used="hyperclova",
                memory_used=use_memory
            )
        raise Exception("Empty response from HyperCLOVA")
    
    async def _try_ollama(
        self,
        system_prompt: str,
        user_message: str,
        memory: Optional[ConversationBufferWindowMemory],
        use_memory: bool
    ) -> ChatResponse:
        """Ollama ì‹œë„"""
        logger.info("Trying Ollama Cloud...")
        
        if use_memory and memory:
            content = await self.ollama.generate_with_memory(
                system_prompt, user_message, memory
            )
        else:
            # ë©”ëª¨ë¦¬ ì—†ì´ ë‹¨ìˆœ ìƒì„±
            llm = self.ollama.get_llm()
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{input}")
            ])
            chain = prompt | llm | StrOutputParser()
            content = await chain.ainvoke({"input": user_message})
        
        if content and content.strip():
            logger.info("Ollama response successful")
            return ChatResponse(
                content=content.strip(),
                model_used="ollama",
                memory_used=use_memory
            )
        raise Exception("Empty response from Ollama")
    
    def _get_fallback_response(self, character_id: str) -> ChatResponse:
        """í´ë°± ì‘ë‹µ"""
        logger.info("Using fallback response")
        responses = FALLBACK_RESPONSES.get(character_id, FALLBACK_RESPONSES['char_1'])
        content = random.choice(responses)
        return ChatResponse(
            content=content,
            model_used="fallback",
            memory_used=False
        )
    
    async def generate_diary_draft(
        self,
        messages: List[str],
        provider: str = "auto"
    ) -> DiaryDraft:
        """ì¼ê¸° ì´ˆì•ˆ ìƒì„±"""
        if not messages:
            return DiaryDraft(
                title="ì˜¤ëŠ˜ì˜ í•˜ë£¨",
                emotion="neutral",
                content="ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ë˜ëŒì•„ë³´ë©° ê¸°ë¡í•´ë³´ì„¸ìš”."
            )
        
        system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì±„íŒ… ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•œ ì¼ê¸° ì´ˆì•ˆì„ ì‘ì„±í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "title": "ì¼ê¸° ì œëª© (5-10ì)",
  "emotion": "happy/sad/anxious/calm/excited/tired/neutral ì¤‘ í•˜ë‚˜",
  "content": "ì¼ê¸° ë‚´ìš© (2-3ë¬¸ì¥, ì‚¬ìš©ì ê´€ì ì˜ 1ì¸ì¹­)"
}"""
        
        user_content = f"ì˜¤ëŠ˜ ë‚˜ëˆˆ ëŒ€í™” ë‚´ìš©:\n{chr(10).join(messages)}\n\nì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ê¸° ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
        
        # ì œê³µìë³„ ì²˜ë¦¬
        if provider == "hyperclova" or (provider == "auto" and self.hyperclova.is_available()):
            try:
                draft = await self._generate_diary_hyperclova(system_prompt, user_content)
                if draft:
                    return draft
            except Exception as e:
                logger.error(f"HyperCLOVA diary generation failed: {e}")
        
        if provider == "ollama" or (provider == "auto" and self.ollama.is_available()):
            try:
                draft = await self._generate_diary_ollama(system_prompt, user_content)
                if draft:
                    return draft
            except Exception as e:
                logger.error(f"Ollama diary generation failed: {e}")
        
        # í´ë°±
        return self._generate_fallback_diary(messages)
    
    async def _generate_diary_hyperclova(self, system_prompt: str, user_content: str) -> Optional[DiaryDraft]:
        """HyperCLOVAë¡œ ì¼ê¸° ìƒì„±"""
        import httpx
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                self.hyperclova.endpoint,
                headers={
                    'X-NCP-CLOVASTUDIO-API-KEY': self.hyperclova.api_key,
                    'X-NCP-APIGW-API-KEY': self.hyperclova.apigw_key,
                    'Content-Type': 'application/json',
                },
                json={
                    'messages': [
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': user_content}
                    ],
                    'topP': 0.8,
                    'maxTokens': 512,
                    'temperature': 0.7
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data.get('result', {}).get('message', {}).get('content', '')
                draft_data = json.loads(content)
                return DiaryDraft(**draft_data)
        return None
    
    async def _generate_diary_ollama(self, system_prompt: str, user_content: str) -> Optional[DiaryDraft]:
        """Ollamaë¡œ ì¼ê¸° ìƒì„±"""
        llm = self.ollama.get_llm()
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}")
        ])
        chain = prompt | llm | StrOutputParser()
        result = await chain.ainvoke({"input": user_content})
        draft_data = json.loads(result)
        return DiaryDraft(**draft_data)
    
    def _generate_fallback_diary(self, messages: List[str]) -> DiaryDraft:
        """í´ë°± ì¼ê¸° ìƒì„±"""
        all_text = ' '.join(messages).lower()
        
        emotion = 'neutral'
        title = 'ì˜¤ëŠ˜ì˜ í•˜ë£¨'
        
        if any(word in all_text for word in ['ì¢‹', 'í–‰ë³µ', 'ê¸°ì¨', 'ì¦ê±°']):
            emotion = 'happy'
            title = 'ê¸°ë¶„ ì¢‹ì€ í•˜ë£¨'
        elif any(word in all_text for word in ['í˜ë“¤', 'ìŠ¬í”„', 'ìš°ìš¸', 'ì†ìƒ']):
            emotion = 'sad'
            title = 'í˜ë“¤ì—ˆë˜ í•˜ë£¨'
        elif any(word in all_text for word in ['ë¶ˆì•ˆ', 'ê±±ì •', 'ê¸´ì¥']):
            emotion = 'anxious'
            title = 'ë¶ˆì•ˆí–ˆë˜ í•˜ë£¨'
        elif any(word in all_text for word in ['í‰ì˜¨', 'í¸ì•ˆ', 'ì°¨ë¶„']):
            emotion = 'calm'
            title = 'í‰ì˜¨í•œ í•˜ë£¨'
        elif any(word in all_text for word in ['ì„¤ë ˆ', 'ê¸°ëŒ€', 'ì‹ ë‚˜']):
            emotion = 'excited'
            title = 'ì„¤ë ˆëŠ” í•˜ë£¨'
        elif any(word in all_text for word in ['í”¼ê³¤', 'ì§€ì¹¨', 'í˜', 'ì¡¸ë ¤']):
            emotion = 'tired'
            title = 'í”¼ê³¤í•œ í•˜ë£¨'
        
        content = ' '.join(messages[:3])[:150]
        if len(' '.join(messages)) > 150:
            content += '...'
        
        return DiaryDraft(title=title, emotion=emotion, content=content)


# AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
ai_service = AIService()

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.get("/")
async def root():
    return {
        "service": "Wave AI Service",
        "version": "1.0.0",
        "status": "running",
        "features": ["langchain", "memory", "multi-provider"]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "providers": {
            "hyperclova": {
                "available": ai_service.hyperclova.is_available(),
                "configured": bool(os.getenv('NAVER_CLOVA_API_KEY') and os.getenv('NAVER_CLOVA_APIGW_KEY'))
            },
            "ollama": {
                "available": ai_service.ollama.is_available(),
                "configured": bool(os.getenv('OLLAMA_CLOUD_API_KEY'))
            }
        },
        "memory_sessions": len(memory_store)
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì±„íŒ… ì‘ë‹µ ìƒì„±
    
    Args:
        provider: "hyperclova", "ollama", "auto" (ê¸°ë³¸ê°’)
        use_memory: ë©”ëª¨ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """
    try:
        # user_idëŠ” ì‹¤ì œë¡œëŠ” ì¸ì¦ í† í°ì—ì„œ ì¶”ì¶œí•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” character_id ì¡°í•©ìœ¼ë¡œ ì‚¬ìš©
        user_id = f"user_{request.character_id}"
        
        response = await ai_service.generate_response(
            request.character_id,
            request.messages,
            request.profile,
            provider=request.provider,
            use_memory=request.use_memory,
            user_id=user_id
        )
        return response
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/diary/generate", response_model=DiaryDraft)
async def generate_diary(request: DiaryGenerateRequest):
    """ì¼ê¸° ì´ˆì•ˆ ìƒì„±
    
    Args:
        provider: "hyperclova", "ollama", "auto" (ê¸°ë³¸ê°’)
    """
    try:
        draft = await ai_service.generate_diary_draft(
            request.messages,
            provider=request.provider
        )
        return draft
    except Exception as e:
        logger.error(f"Diary generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/memory/clear/{user_id}/{character_id}")
async def clear_memory(user_id: str, character_id: str):
    """íŠ¹ì • ì‚¬ìš©ì-ìºë¦­í„°ì˜ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
    key = f"{user_id}:{character_id}"
    if key in memory_store:
        del memory_store[key]
        return {"status": "success", "message": f"Memory cleared for {key}"}
    return {"status": "not_found", "message": f"No memory found for {key}"}

@app.get("/memory/stats")
async def memory_stats():
    """ë©”ëª¨ë¦¬ í†µê³„"""
    stats = {}
    for key, memory in memory_store.items():
        history = memory.load_memory_variables({}).get("chat_history", [])
        stats[key] = {
            "message_count": len(history),
            "window_size": memory.k
        }
    return {"total_sessions": len(memory_store), "sessions": stats}

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
