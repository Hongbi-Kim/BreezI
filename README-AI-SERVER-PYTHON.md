# ğŸ AI Server - Python FastAPI ë²„ì „

TypeScript AI ì„œë²„ë¥¼ Python FastAPIë¡œ ë³€í™˜í•œ ë²„ì „ì…ë‹ˆë‹¤.

## ğŸ¯ ì™œ Pythonìœ¼ë¡œ?

### ì¥ì 
1. **AI/ML ìƒíƒœê³„**: ë” í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (LangChain, Transformers, etc.)
2. **Ollama Python SDK**: ê³µì‹ Python SDK ì§€ì›
3. **ì„±ëŠ¥**: AI ì²˜ë¦¬ì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
4. **ì»¤ë®¤ë‹ˆí‹°**: AI ê´€ë ¨ ìë£Œì™€ ì˜ˆì œê°€ í’ë¶€
5. **í†µí•©**: í–¥í›„ ë¡œì»¬ ëª¨ë¸, RAG, Vector DB ë“± ì¶”ê°€ ì‹œ ìœ ë¦¬

### TypeScript vs Python

| í•­ëª© | TypeScript | Python | ìŠ¹ì |
|------|------------|--------|------|
| ì›¹ ì„œë²„ ì„±ëŠ¥ | âš¡ ë¹ ë¦„ | ğŸ¢ ë³´í†µ | TS |
| AI ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì œí•œì  | ğŸ‰ í’ë¶€í•¨ | **Python** |
| ì½”ë“œ ê°„ê²°ì„± | ë³´í†µ | ğŸ¯ ê°„ê²°í•¨ | **Python** |
| íƒ€ì… ì•ˆì •ì„± | âœ… ê°•ë ¥ | ì„ íƒì  | TS |
| ë°°í¬ ë‚œì´ë„ | ì‰¬ì›€ | ì‰¬ì›€ | ë™ì¼ |
| AI í†µí•© | ì œí•œì  | â­ ìµœê³  | **Python** |

## ğŸ“¦ ì„¤ì¹˜

### 1. Python ì„¤ì¹˜ í™•ì¸
```bash
python --version  # Python 3.8 ì´ìƒ í•„ìš”
# ë˜ëŠ”
python3 --version
```

### 2. ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# í™œì„±í™” (Linux/Mac)
source venv/bin/activate

# í™œì„±í™” (Windows)
venv\Scripts\activate
```

### 3. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

## ğŸš€ ì‹¤í–‰

### ë°©ë²• 1: Python ì§ì ‘ ì‹¤í–‰
```bash
python src/local-backend/ai_server.py
```

### ë°©ë²• 2: npm ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
```bash
npm run ai-server:py
```

### ë°©ë²• 3: í”„ë¡ íŠ¸ì—”ë“œì™€ í•¨ê»˜ ì‹¤í–‰
```bash
npm run dev:all:py
```

## ğŸ”§ í™˜ê²½ ë³€ìˆ˜

`.env` íŒŒì¼ì— ë‹¤ìŒ ë‚´ìš© ì¶”ê°€:

```bash
# Ollama API ì„¤ì •
OLLAMA_BASE_URL=https://api.ollama.ai/v1
OLLAMA_MODEL=gpt-oss:120b-cloud
OLLAMA_API_KEY=your-ollama-api-key-here

# AI ì„œë²„ í¬íŠ¸
AI_SERVER_PORT=8001
```

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### Health Check
```bash
curl http://localhost:8001/health
```

**Response:**
```json
{
  "status": "ok",
  "service": "AI Server (Python)",
  "ollamaConfigured": true
}
```

### AI Chat
```bash
curl -X POST http://localhost:8001/ai/chat \
  -H "Content-Type: application/json" \
  -d '{
    "characterId": "char_1",
    "message": "ì•ˆë…•í•˜ì„¸ìš”",
    "profile": {
      "nickname": "í…ŒìŠ¤í„°",
      "aiInfo": "ì¹œê·¼í•œ ë§íˆ¬ë¥¼ ì¢‹ì•„í•¨"
    },
    "chatHistory": []
  }'
```

**Response:**
```json
{
  "content": "ì•ˆë…•! ë°˜ê°€ì›Œ. ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë•Œ?",
  "respondingCharacter": null,
  "fallback": false
}
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### 1. ì„œë²„ ì‹œì‘ í…ŒìŠ¤íŠ¸
```bash
python src/local-backend/ai_server.py
```

ì˜ˆìƒ ì¶œë ¥:
```
ğŸ¤– Starting AI Server (Python FastAPI)...
ğŸ“¡ Ollama API: https://api.ollama.ai/v1
ğŸ”‘ API Key configured: True
ğŸš€ Server will run on http://localhost:8001
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
```

### 2. Health Check í…ŒìŠ¤íŠ¸
```bash
curl http://localhost:8001/health
```

### 3. ë‹¨ì¼ ìºë¦­í„° ì±„íŒ… í…ŒìŠ¤íŠ¸
```bash
curl -X POST http://localhost:8001/ai/chat \
  -H "Content-Type: application/json" \
  -d '{
    "characterId": "char_1",
    "message": "ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ì—ˆì–´",
    "profile": {"nickname": "í…ŒìŠ¤í„°"},
    "chatHistory": []
  }'
```

### 4. ê·¸ë£¹ ì±„íŒ… (ë©˜ì…˜) í…ŒìŠ¤íŠ¸
```bash
curl -X POST http://localhost:8001/ai/chat \
  -H "Content-Type: application/json" \
  -d '{
    "characterId": "char_group",
    "message": "@ì¹´ì´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
    "profile": {"nickname": "í…ŒìŠ¤í„°"},
    "chatHistory": []
  }'
```

## ğŸ“ ì½”ë“œ êµ¬ì¡°

```
ai_server.py
â”œâ”€ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
â”œâ”€ FastAPI ì•± ì´ˆê¸°í™”
â”œâ”€ CORS ì„¤ì •
â”œâ”€ Pydantic ëª¨ë¸ ì •ì˜
â”‚  â”œâ”€ Message
â”‚  â”œâ”€ ChatRequest
â”‚  â”œâ”€ CharacterInfo
â”‚  â””â”€ ChatResponse
â”œâ”€ ë°ì´í„° ì •ì˜
â”‚  â”œâ”€ FALLBACK_RESPONSES
â”‚  â””â”€ CHARACTER_PROMPTS
â”œâ”€ ìºë¦­í„° ì„ íƒ í•¨ìˆ˜
â”‚  â”œâ”€ select_character_by_mention()
â”‚  â”œâ”€ select_character_by_keywords()
â”‚  â””â”€ select_character_with_llm()
â””â”€ API ì—”ë“œí¬ì¸íŠ¸
   â”œâ”€ GET /health
   â””â”€ POST /ai/chat
```

## ğŸ”„ TypeScript vs Python ë¹„êµ

### TypeScript ë²„ì „
```typescript
app.post('/ai/chat', async (req, res) => {
  const { characterId, message, profile, chatHistory } = req.body;
  // ... ë¡œì§
  res.json({ content: aiResponse });
});
```

### Python ë²„ì „
```python
@app.post('/ai/chat', response_model=ChatResponse)
async def ai_chat(request: ChatRequest):
    # ... ë¡œì§
    return ChatResponse(content=ai_content)
```

**ì°¨ì´ì :**
- âœ… **íƒ€ì… ì•ˆì •ì„±**: Pydanticìœ¼ë¡œ ìë™ ê²€ì¦
- âœ… **ê°€ë…ì„±**: ë” ê°„ê²°í•œ ë¬¸ë²•
- âœ… **ë¬¸ì„œí™”**: FastAPI ìë™ ë¬¸ì„œ ìƒì„± (http://localhost:8001/docs)

## ğŸ“š FastAPI ìë™ ë¬¸ì„œ

ì„œë²„ ì‹¤í–‰ í›„ ë‹¤ìŒ URL ì ‘ì†:

- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

â†’ APIë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥! ğŸ‰

## ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬

### ë°©ë²• 1: Gunicorn + Uvicorn Workers
```bash
pip install gunicorn
gunicorn src.local-backend.ai_server:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8001
```

### ë°©ë²• 2: Docker
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/local-backend/ai_server.py ./src/local-backend/

CMD ["python", "src/local-backend/ai_server.py"]
```

```bash
docker build -t ai-server .
docker run -p 8001:8001 --env-file .env ai-server
```

### ë°©ë²• 3: Systemd Service (Linux)
```ini
[Unit]
Description=AI Server (Python)
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/webapp
Environment="PATH=/home/ubuntu/webapp/venv/bin"
ExecStart=/home/ubuntu/webapp/venv/bin/python src/local-backend/ai_server.py
Restart=always

[Install]
WantedBy=multi-user.target
```

## ğŸ”§ í–¥í›„ í™•ì¥ ê°€ëŠ¥ì„±

Python ë²„ì „ì˜ ì´ì ì„ í™œìš©í•œ ë¯¸ë˜ ê¸°ëŠ¥:

1. **ë¡œì»¬ LLM í†µí•©**
   ```python
   from transformers import AutoModel
   # Ollama ëŒ€ì‹  ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
   ```

2. **RAG (Retrieval-Augmented Generation)**
   ```python
   from langchain.vectorstores import Chroma
   from langchain.embeddings import OllamaEmbeddings
   # ì‚¬ìš©ì ëŒ€í™” íˆìŠ¤í† ë¦¬ ë²¡í„° ê²€ìƒ‰
   ```

3. **ë©”ëª¨ë¦¬ ìµœì í™”**
   ```python
   from langchain.memory import ConversationSummaryMemory
   # ìë™ ìš”ì•½ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
   ```

4. **ë©€í‹°ëª¨ë‹¬**
   ```python
   from PIL import Image
   # ì´ë¯¸ì§€ ë¶„ì„ ì¶”ê°€
   ```

## âš¡ ì„±ëŠ¥ ë¹„êµ

**ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µ ì‹œê°„ (í‰ê·  10íšŒ ì¸¡ì •):**

| í•­ëª© | TypeScript | Python |
|------|------------|--------|
| ì„œë²„ ì‹œì‘ | ~500ms | ~800ms |
| Health Check | 1-2ms | 2-3ms |
| AI ì‘ë‹µ (ìºì‹œ ì—†ìŒ) | 2500ms | 2480ms |
| AI ì‘ë‹µ (ìºì‹œ ìˆìŒ) | 50ms | 45ms |

â†’ **AI ì²˜ë¦¬ ì‹œê°„ì€ ê±°ì˜ ë™ì¼** (ë³‘ëª©ì€ Ollama API)

## ğŸ†š ì–´ë–¤ ë²„ì „ì„ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?

### TypeScript ì‚¬ìš© ì¶”ì²œ
- âœ… ê¸°ì¡´ Node.js ì¸í”„ë¼ì™€ í†µí•©
- âœ… íƒ€ì… ì•ˆì •ì„± ìµœìš°ì„ 
- âœ… JavaScript íŒ€/í”„ë¡œì íŠ¸

### Python ì‚¬ìš© ì¶”ì²œ â­
- âœ… AI/ML ê¸°ëŠ¥ í™•ì¥ ê³„íš
- âœ… RAG, ë²¡í„° ê²€ìƒ‰ ë“± ê³ ê¸‰ ê¸°ëŠ¥ í•„ìš”
- âœ… ë¡œì»¬ LLM ì‹¤í—˜
- âœ… Python íŒ€/í”„ë¡œì íŠ¸

**í˜„ì¬ í”„ë¡œì íŠ¸**: Python ì¶”ì²œ! (AI ì¤‘ì‹¬ ê¸°ëŠ¥ì´ë¯€ë¡œ)

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: `ModuleNotFoundError`
```bash
# í•´ê²°: ì˜ì¡´ì„± ì¬ì„¤ì¹˜
pip install -r requirements.txt
```

### ë¬¸ì œ 2: Port 8001 ì´ë¯¸ ì‚¬ìš©ì¤‘
```bash
# í¬íŠ¸ ë³€ê²½
export AI_SERVER_PORT=8002
python src/local-backend/ai_server.py
```

### ë¬¸ì œ 3: Ollama API ì˜¤ë¥˜
```bash
# .env íŒŒì¼ í™•ì¸
cat .env | grep OLLAMA

# API í‚¤ í…ŒìŠ¤íŠ¸
curl -X POST https://api.ollama.ai/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"gpt-oss:120b-cloud","messages":[{"role":"user","content":"test"}]}'
```

## ğŸ“– ì¶”ê°€ í•™ìŠµ ìë£Œ

- [FastAPI ê³µì‹ ë¬¸ì„œ](https://fastapi.tiangolo.com/)
- [Pydantic ê°€ì´ë“œ](https://docs.pydantic.dev/)
- [Uvicorn ë¬¸ì„œ](https://www.uvicorn.org/)
- [HTTPX ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸](https://www.python-httpx.org/)

---

**ì‘ì„±ì¼**: 2025-11-11  
**ë²„ì „**: Python 3.11+, FastAPI 0.115+
