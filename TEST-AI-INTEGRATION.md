# AI Server Integration Test Plan

## Phase 2 Complete! ğŸ‰

### What Changed

1. **Removed from Supabase Functions** (`src/supabase/functions/server/index.tsx`):
   - `selectCharacterByMention()` - ë©˜ì…˜ ê°ì§€ ë¡œì§
   - `selectCharacterWithLLM()` - LLM ê¸°ë°˜ ìºë¦­í„° ë¼ìš°íŒ…
   - `selectCharacterForGroupChat()` - í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ë¼ìš°íŒ…
   - `getAIResponseWithMemory()` - AI ì‘ë‹µ ìƒì„±
   - `generateSummaryWithOllama()` - ëŒ€í™” ìš”ì•½ ìƒì„±
   - `initializeMemory()` - LangChain ë©”ëª¨ë¦¬ ì´ˆê¸°í™”

2. **Added to Supabase Functions**:
   - Simple `fetch()` call to `AI_SERVER_URL/ai/chat`
   - Fallback response mechanism when AI server is unavailable
   - Simplified chat history preparation (`prepareChatHistory()`)

3. **Result**:
   - Supabase Functions: ~200 lines shorter, cleaner, faster
   - AI Server: Contains all complex AI logic
   - Clear separation of concerns

## Testing Steps

### 1. Start Both Servers

```bash
# Terminal 1: Frontend + Supabase Functions
npm run dev

# Terminal 2: AI Server
npm run ai-server

# OR run both at once:
npm run dev:all
```

### 2. Test Single Character Chat

1. Open browser: http://localhost:5173
2. Login with test account
3. Click on any character (ë£¨ë¯¸, ì¹´ì´, ë ˆì˜¤)
4. Send a test message: "ì•ˆë…•í•˜ì„¸ìš”"
5. Verify AI response appears

**Expected Console Logs (Supabase)**:
```
Processing chat for char_1... (Total messages: X)
Calling AI server at http://localhost:8001/ai/chat
âœ… AI server response received
```

**Expected Console Logs (AI Server)**:
```
ğŸ“¥ Received chat request for character: char_1
ğŸ“ Message: "ì•ˆë…•í•˜ì„¸ìš”"
ğŸ“œ Chat history length: X messages
âœ… AI response generated
```

### 3. Test Group Chat

1. Click on "ë£¨ë¯¸+ì¹´ì´+ë ˆì˜¤" group chat
2. Send message WITHOUT mention: "ì˜¤ëŠ˜ í˜ë“  í•˜ë£¨ì˜€ì–´"
3. Verify appropriate character responds (likely ë£¨ë¯¸)

**Expected**: LLM routing selects emotional support character

4. Send message WITH mention: "@ì¹´ì´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?"
5. Verify ì¹´ì´ responds

**Expected**: Mention detection overrides LLM routing

### 4. Test Fallback (AI Server Down)

1. Stop AI server (Ctrl+C in Terminal 2)
2. Send a message
3. Verify fallback response appears

**Expected Console Logs**:
```
âŒ AI server call failed: Error: fetch failed
Using fallback response...
```

**Expected Response**: Random fallback response from predefined list

### 5. Test Performance

Compare response times:
- **Before** (old system): ~2-5 seconds
- **After** (new system): Similar (AI processing time unchanged)
- **Advantage**: Easier to scale, debug, and maintain

## Success Criteria

âœ… Single character chat works
âœ… Group chat with LLM routing works
âœ… Group chat with mentions works
âœ… Fallback responses work when AI server is down
âœ… No errors in console (frontend or backend)
âœ… Response times acceptable

## Next Steps (Future)

1. **Production Deployment**:
   - Deploy AI server to cloud (AWS/GCP/DigitalOcean)
   - Update `AI_SERVER_URL` environment variable in Supabase
   - Add authentication between Supabase and AI server

2. **Add Summary Endpoint**:
   - Create `/ai/summary` endpoint in AI server
   - Call from Supabase when message count exceeds threshold

3. **Monitoring**:
   - Add logging/metrics to AI server
   - Monitor AI server health from Supabase
   - Alert when AI server is unreachable

## Troubleshooting

### AI Server Connection Refused
**Problem**: `Error: connect ECONNREFUSED 127.0.0.1:8001`
**Solution**: Ensure AI server is running (`npm run ai-server`)

### Ollama API Errors
**Problem**: AI server returns 401/403 from Ollama
**Solution**: Check `.env` file has valid `OLLAMA_API_KEY`

### Fallback Responses Always Triggered
**Problem**: Never gets AI responses
**Solution**: 
1. Check AI server console for errors
2. Verify `AI_SERVER_URL` in Supabase Functions
3. Test AI server directly: `curl http://localhost:8001/health`

